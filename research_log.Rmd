---
title: "Research Log Project Machine Learning"
subtitle: "Diagnosing malignancy of breast masses using Machine Learning"
output: pdf_document
header-includes:
   - \pagenumbering{gobble}
   - \usepackage{longtable}
   - \usepackage{hyperref}
---

```{r setup, include = FALSE}
# Set code chunks visibility in pdf output to true
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(fig.align = "center")
```

<!-- (Front page) -->
\vspace{350pt}

\hfill \textbf{Student}: Vincent Talen  

\hfill \textbf{Student number}: 389015  

\hfill \textbf{Class}: BFV3

\hfill \textbf{Study}: Bioinformatics

\hfill \textbf{Institute}: Institute for Life Science & Technology

\hfill \textbf{Teachers}: Dave Langers (LADR) and Bart Barnard (BABA)

\hfill \textbf{Date}: `r Sys.Date()`

\newpage
<!-- (Table of contents) -->
\setcounter{secnumdepth}{2}
\tableofcontents
\pagenumbering{arabic}


\newpage
<!-- (Setting up R) -->
# Preparing R environment
For the data analysis and further processes multiple libraries are needed, they are loaded in here.
A few other options/settings are configured and used scripts also loaded.

```{r preparing r environment, message = FALSE}
# Load required packages from vector using invisible and lapply
packages <- c("tidyverse", "pander", "ggplot2", "data.table", "ggpubr")
invisible(lapply(packages, library, character.only = TRUE))
remove(packages) # Drop variable since it will not be used again

# Source functions
source("split_violin_plot.R")

# Disable printing 'table continues' lines between split sections
pander::panderOptions("table.continues", "")
# Change affix after table caption if it's a split table
pander::panderOptions("table.continues.affix", "(table continues below)")
```

\newpage
<!-- (Exploratory Data Analysis) -->
# Exploratory Data Analysis
## About the chosen data set
### Origin of the data
The data set that is used is the *[Wisconsin Breast Cancer (Diagnostic) Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)*, which is publicly available from the UCI Machine Learning Repository. There are two published research articles, from the same team of researchers, where the data set was first used, namely \cite{street93} and \cite{mangasarian95}. The samples for the data were collected from 569 patients at the University of Wisconsin Hospital with the goal of creating a machine learning model that was faster, improved the correctness and increased the objectivity of the diagnosis process of breast cancer. 


### Collection of the data
The data was gathered by first collecting the fine needle aspirates (FNA), which are expressed on a glass slide and stained. The images were generated by a color video camera mounted on top of a microscope, that projected the images with a 63x objective and 2.5x ocular into the camera. The images were then captured by a color frame grabber board as a 512x480, 8-bit-per-pixel Targa file.  
The digitized image is then analyzed in the program Xcyt (custom made by Nick Street). First the user marks approximate initial boundaries of the nuclei and then the actual boundaries are further defined with an active contour model known as "Snake". In the end the snake reaches a point where it's curve accurately corresponds to the boundary of a cell nucleus. From the snake-generated cell nuclei boundaries 10 features are extracted, these are numerically modeled such that larger values will typically indicate a higher likelihood of malignancy. 

The ten features that are extracted for each cell nucleus are the following:

 1. Radius (mean of distances from center to points on the perimeter)
 2. Texture (standard deviation of gray-scale values)
 3. Perimeter (the total distance between all the points of the snake-generated boundary)
 4. Area (the nuclear area is the sum of pixels on the interior, with half of the pixels of the perimeter)
 5. Smoothness (local variation in radius lengths)
 6. Compactness (perimeter^2 / area - 1.0)
 7. Concavity (severity of concave portions of the contour)
 8. Concave points (number of concave portions of the contour)
 9. Symmetry (difference in length of perpendicular lines to the longest chord through the center, in both directions)
10. Fractal dimension (approximated using Mandelbrot's "coastline approximation" - 1)

For each feature and for every image, three final values were computed and saved to the data set, namely the mean, standard error and the extreme (largest) value.


### Data structure and codebook
The data set has 569 instances/rows with 32 columns, an ID column, a classification column with the diagnosis (benign or malignant) and 30 columns describing the nuclei boundaries (10x mean/extreme/se).  
Because the data set itself does not come with an annotated header with column names, a codebook has been manually made. This codebook has the abbreviated column name, the full column name, the data type and a description for each feature/column.

Below is an overview of the columns in the data set, shown using the contents of the codebook after it has been loaded in:
```{r import codebook}
codebook <- readr::read_delim("data/codebook.txt", delim = "|", show_col_types = FALSE)

# Pretty print the codebook (without descriptions) using pander
pander::pander(codebook[, 1:3], style = "rmarkdown", caption = "Overview of created codebook excluding descriptions")
```

As can be seen, all the features are of the type double except the main diagnosis classification factor.

\newpage
## Loading in the data set
The data will be loaded in with the `read_csv` function from the `readr` package, this function returns the data as a tibble data frame.
This function allows a vector with column names to be given with an argument, the names from the column `Column Name` of the codebook will be used.
```{r load data from file}
data <- readr::read_csv("data/wdbc.data", col_names = codebook[[1]], show_col_types = FALSE)

# Print the amount of samples and columns
cat("Amount of samples:", dim(data)[1], "\tColumns in dataframe:", dim(data)[2], "\n")
```
The amount of samples and columns are as expected, so in this aspect the data set has been read correctly. However, what is more important is if the values are read correctly, since the values are that what is actually going to be used.

```{r remove unwanted tibble attributes, include=FALSE}
attr(data, "spec") <- NULL; attr(data, "problems") <- NULL
```
```{r print data structure}
str(data)
```
Luckily it looks like the values for every column have correctly been read, but there is one thing that could still be changed; the diagnosis column. It would be more helpful if diagnosis was a factor and not just a character column.

```{r data distribution, out.width = "75%", fig.cap = "Barplot showing distribution of diagnosis classification labels"}
data$diagnosis <- factor(data$diagnosis, labels = c("Benign", "Malignant"))

ggplot(data, aes(x = diagnosis, fill = diagnosis)) + geom_bar() + 
  geom_text(stat='count', aes(label = after_stat(count)), nudge_y = -15) + 
  scale_fill_hue(direction = 1, h.start = 180) +
  ggtitle("Barplot of diagnosis column")
```
There are more benign cases than malignant, this means the data set is not entirely balanced and could cause bias if not handled correctly.

The data set has the `id` column, this column is not needed for the analysis that will be performed so it will therefore be dropped from the data frame. The `id` column could even cause small a hiccup when creating the machine learning model, since all the values are unique the model could use that as the only feature to predict the classification label. This would of course not work since unseen data will not have the same id's as the data used for training the machine learning algorithm.
```{r drop id column}
data <- dplyr::select(data, -id)
colnames(data)
```


\newpage
## Data inspection
### Checking for missing values
It is important to have a good understanding of what the data is like, for example how the data is distributed and if there is data corruption. A few things that should be checked are if there are any outliers present, any data is skewed (an asymmetry in data distribution) or if there is any missing data.
```{r check missing values}
cat("Missing values:", any(is.na(data)))
```
Luckily there are no missing values in the data set. But now it is good to get an idea of what the values of the columns look like, what ranges do their values fall in? This can be done with the function `summary()` for all columns at the same time, it will create a basic statistics overview about the columns.

### Overall data summary
```{r show summery of data}
pander::pander(summary(data), caption = "Summary with basic statistics about the data colums")
```
By glancing over the summary created above a few things can be noticed and questions can arise, for example the `area_mean`, `concave_pts_mean`, `radius_se`, `perimeter_se` and `area_worst` columns. These, among other columns, have a wide range of values with their minimum or maximum values far from the quantiles or median.  
This could mean that there are outliers in the data, the questions that arise because of this is if these points are actually outliers and if they should be excluded from the data. It can however not be determined with just the summary above if these are actually outliers and if they need to be removed. 

### Univariate analysis
To check if points are outliers the data needs to be visualized, this can be done by creating a box plot and/or density plot for each of the columns.
Creating them separately for all 30 feature columns would result in 60 plots, which is a lot and perhaps too many. To reduce the amount of total plots, the density plots will instead be visualized using violin plots with the box plots inside of them. Because printing all the data of a feature column in a single box or violin does not show what is desired, the plots will be split on the diagnosis classification factor.

There is not a function in `ggplot` to create these split violin plots, the source code file `split_violin_plot.R` contains a function that uses ggplot as a basis to create split violin plots. Then, using this function from the source file, another function is created that assembles the full plot including the box plot, plot title and themes. After this a list of column names can be given and a plot will be generated for each of them and they will the be arranged with a common legend to the output file. So there is a clearer overview where the data can be compared the plots for the feature columns will be split on their specification, resulting in three arranged layouts with the mean-, standard error- and worst plots.

```{r create plot functions}
createFeaturePlot <- function(col_name, figure_tab) {
  plot <- ggplot(data, aes(x = "", y = !!sym(col_name), fill = diagnosis)) +
    geom_split_violin(alpha = 0.6, trim = FALSE) +
    geom_boxplot(width = 0.2, alpha = 0.6, fatten = NULL, show.legend = F) +
    stat_summary(fun.data = "mean_se", geom = "pointrange", show.legend = F, 
                 position = position_dodge(0.2), size = 0.3) +
    scale_fill_brewer(palette = "Dark2", name = "Diagnosis:") +
    ggtitle(dplyr::filter(codebook, `Column Name` == col_name)$`Full Name`) +
    theme_minimal() + labs(y = NULL, x = NULL, tag = figure_tab) + 
    theme(plot.title = element_text(size = 9, face = "bold"))#, hjust = 0.5))
  return(list(plot))
}

createPlotGrid <- function(extension) {
  desired_col_names <- colnames(data)[colnames(data) %like% extension]
  figure_tabs <- letters[1:length(desired_col_names)]
  # Create plots and put them in a list
  plot_list <- mapply(createFeaturePlot, desired_col_names, figure_tabs)
  # Print the plots in an arranged grid with the legend at the bottom
  ggpubr::ggarrange(plotlist = plot_list, ncol = 4, nrow = 3,
                    common.legend = TRUE, legend = "bottom")
}
```

```{r _mean plots, fig.width = 7.5, fig.height = 9.5, fig.cap = "Split violin plots showing density joined with box plots showing quartiles and outliers, for all 'mean' feature columns"}
# Create split violin plots for all mean feature columns
ggpubr::annotate_figure(
  createPlotGrid("_mean"), 
  top = text_grob("Grid of violin plots with boxplots for each 'mean' feature column",
                  face = "bold", size = 14))
```

```{r _se plots, fig.width = 7.5, fig.height = 9.5, fig.cap = "Split violin plots showing density joined with box plots showing quartiles and outliers, for all 'standard error' feature columns"}
# Create split violin plots for all standard error feature columns
ggpubr::annotate_figure(
  createPlotGrid("_se"), 
  top = text_grob("Grid of violin plots with boxplots for each 'standard error' feature column",
                  face = "bold", size = 14))
```

```{r _worst plots, fig.width = 7.5, fig.height = 9.5, fig.cap = "Split violin plots showing density joined with box plots showing quartiles and outliers, for all 'worst/extreme' feature columns"}
# Create split violin plots for all worst/extreme feature columns
ggpubr::annotate_figure(
  createPlotGrid("_worst"), 
  top = text_grob("Grid of violin plots with boxplots for each 'worst' feature column",
                  face = "bold", size = 14))
```

Looking at the resulting plots it can easily be seen if there is a distinctive correlation between the classification factor and the values of feature columns.
The couple of columns that stand out the most are the `Radius`, `Perimeter`, `Area`, `Concavity` and `Concave Points` feature columns, these all seem to have a clear distinction between the diagnosis classification.  
Another thing that should be noted is that in most of the standard error columns a lot of outliers can be seen, for now nothing will be done with these but it is a good thing to know might problems arise later on.

\newpage
### Bivariate analysis
Trying to visually identify correlations between features using the above plots is not very exact, a better way is to create a correlation matrix. In the correlation matrix all the possible pairs of features have their correlation calculated.  
The correlation matrix can then be visualized as a heatmap, where the strength of correlations is represented with colors so they can easily be visually seen.
```{r correlation matrix}
# Create correlation matrix with only the numerical feature columns
cor_matrix <- stats::cor(select(data, -diagnosis))
cor_matrix <- tibble::as_tibble(cor_matrix)

# Insert column with feature names and set it as the left most column
column_names <- colnames(cor_matrix)
cor_matrix <- cor_matrix %>% 
    dplyr::mutate(col_names = all_of(column_names)) %>% 
    dplyr::select(31, 1:30)

# Show first five columns of correlation matrix
pander::pander(head(cor_matrix[1:5], n = 5))
```

Before the correlation matrix can be used to create a heatmap it needs to be converted to long format, this is because of the way R and ggplot read and use data.
```{r long correlation matrix}
# Convert data to long format so a heatmap can be made from it
cor_matrix_long <- tidyr::pivot_longer(data = cor_matrix, cols = all_of(column_names), 
                                       names_to = "variable", values_to = "cor")
# Show what the data looks like now
pander::pander(head(cor_matrix_long, n = 5))
```
In the matrix the features are not physically paired as actual instances, in the long format the features are now saved as pairs in rows with the correlation score. This way the two columns, each with a feature of a pair, can be used as the axes of the heatmap plot.

```{r heatmap, fig.cap = "Heatmap visualizing pairwise correlation matrix of all feature colums"}
ggplot(data = cor_matrix_long, aes(x = col_names, y = variable, fill = cor)) +
  geom_tile() + labs(x = NULL, y = NULL) +
  scale_fill_gradient(high = "purple", low = "white" ) +
  theme(axis.text.x = element_text(angle = 45, hjust=1)) +
  ggtitle("Heatmap of column correlations")
```
One thing that can immediately be noticed and should not be surprising is that the mean-, standard error- and worst feature columns of each nuclei boundary feature, i.e. of the radius, have a high correlation to each other.

When looking at the heatmap it can be seen very clearly that multiple features are heavily correlated together. Next, it could be a good idea to create scatterplots with trend lines of these heavily correlated feature pairs.

### Data clustering
```{r clustering}
# Tree, Scatter plot, PCA plot
```


<!-- (References) -->
\newpage
\begin{thebibliography}{9}

\bibitem{street93}
W.N. Street, W.H. Wolberg and O.L. Mangasarian. (1993), \textit{Nuclear feature extraction for breast tumor diagnosis.}, 1993 International Symposium on Electronic Imaging: Science and Technology, volume 1905, pages 861-870, \url{https://doi.org/10.1117/12.148698} (accessed Sep 16, 2022).

\bibitem{mangasarian95}
O.L. Mangasarian, W.N. Street and W.H. Wolberg. (1995), \textit{Breast cancer diagnosis and prognosis via linear programming}, Operations Research, volume 43, issue 4, pages 570-577, \url{https://doi.org/10.1287/opre.43.4.570} (accessed Sep 17, 2022).
\end{thebibliography}