---
title: "Research Log Project Machine Learning"
subtitle: "Diagnosing malignancy of breast masses using Machine Learning"
output: pdf_document
header-includes:
   - \pagenumbering{gobble}
   - \usepackage{longtable}
   - \usepackage{hyperref}
   - \usepackage[nottoc]{tocbibind}
   - \usepackage[justification=centering]{caption}

knit: (function(inputFile, encoding) {
  rmarkdown::render( inputFile, encoding = encoding, output_dir = here::here() ) })
---

<!-- (Front page) -->
\vspace{350pt}

\hfill \textbf{Student}: Vincent Talen  

\hfill \textbf{Student number}: 389015  

\hfill \textbf{Class}: BFV3

\hfill \textbf{Study}: Bioinformatics

\hfill \textbf{Institute}: Institute for Life Science & Technology

\hfill \textbf{Teachers}: Dave Langers (LADR) and Bart Barnard (BABA)

\hfill \textbf{Date}: `r Sys.Date()`

\newpage
\pagenumbering{roman}
<!-- (Table of contents) -->
\setcounter{secnumdepth}{2}
\renewcommand{\contentsname}{Table of Contents}
\tableofcontents
<!-- (LISTS OF FIGURES AND TABLES) -->
\newpage
\listoffigures
\listoftables
<!-- (MAIN ARTICLE CONTENT PAST THIS) -->
\newpage
\pagenumbering{arabic}

<!-- (Setting up R) -->
# Preparing R environment
Set some options for the code chunks here so they don't have to be set for every chunk separately.  
Also set the directory this markdown file knits and runs from to the project directory.
```{r setup}
# Set code chunk options
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(fig.path = here::here("output/figures/"))
knitr::opts_chunk$set(cache.path = here::here("src/rmd/research_log_cache/"))

# Set directory of this Rmd file to project directory
knitr::opts_knit$set(root.dir = here::here())
```

For the data analysis and further processes multiple libraries are needed, they are loaded in here.
A few other options/settings are configured and used scripts also loaded.
```{r preparing-r-environment, message = FALSE, cache = FALSE}
# Load required packages from vector using invisible and lapply
packages <- c("tidyverse", "pander", "data.table", "RWeka",
              "ggplot2", "ggpubr", "ggbiplot")
invisible(lapply(packages, library, character.only = TRUE))
remove(packages) # Drop variable since it will not be used again

# Source functions
source("src/scripts/split_violin_plot.R")
source("src/scripts/weka_analyser_custom.R")

# Disable printing 'table continues' lines between split sections
pander::panderOptions("table.continues", "")
# Change affix after table caption if it's a split table
pander::panderOptions("table.continues.affix", "(table continues below)")
```

\newpage
<!-- (Exploratory Data Analysis) -->
# Exploratory Data Analysis
## About the chosen dataset
The dataset that is used is the *[Wisconsin Breast Cancer (Diagnostic) Dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)*, which is publicly available from the UCI Machine Learning Repository. There are two published research articles, from the same team of researchers, where the dataset was first used, namely \cite{street93} and \cite{mangasarian95}. The samples for the data were collected from 569 patients at the University of Wisconsin Hospital with the goal of creating a machine learning model that was faster, improved the correctness and increased the objectivity of the diagnosis process of breast cancer. 

### Collection of the data
The data was gathered by first collecting the fine needle aspirates (FNA), which are expressed on a glass slide and stained. The images were generated by a color video camera mounted on top of a microscope, that projected the images with a 63x objective and 2.5x ocular into the camera. The images were then captured by a color frame grabber board as a 512x480, 8-bit-per-pixel Targa file.  
The digitized image is then analyzed in the program Xcyt (custom made by Nick Street). First the user marks approximate initial boundaries of the nuclei and then the actual boundaries are further defined with an active contour model known as "Snake". In the end the snake reaches a point where it's curve accurately corresponds to the boundary of a cell nucleus. From the snake-generated cell nuclei boundaries 10 features are extracted, these are numerically modeled such that larger values will typically indicate a higher likelihood of malignancy. 

The ten features that are extracted for each cell nucleus are the following:

 1. Radius (mean of distances from center to points on the perimeter)
 2. Texture (standard deviation of gray-scale values)
 3. Perimeter (the total distance between all the points of the snake-generated boundary)
 4. Area (the nuclear area is the sum of pixels on the interior, with half of the pixels of the perimeter)
 5. Smoothness (local variation in radius lengths)
 6. Compactness (perimeter^2 / area - 1.0)
 7. Concavity (severity of concave portions of the contour)
 8. Concave points (number of concave portions of the contour)
 9. Symmetry (difference in length of perpendicular lines to the longest chord through the center, in both directions)
10. Fractal dimension (approximated using Mandelbrot's "coastline approximation" - 1)

For every image, three final values were computed for each feature and saved to the dataset, namely the mean, standard error and the extreme (largest) value.


### Data structure and codebook
The dataset has 569 instances/rows with 32 columns, an ID column, a classification column with the diagnosis (benign or malignant) and 30 columns describing the nuclei boundaries (10x mean/extreme/se).  
Because the dataset itself does not come with an annotated header with column names, a codebook has been manually made. This codebook has the abbreviated column name, the full column name, the data type and a description for each feature/column.

Below is an overview of the columns in the dataset, shown using the contents of the codebook after it has been loaded in:
```{r import-codebook}
codebook <- readr::read_delim("data/raw/codebook.txt", delim = "|", show_col_types = FALSE)

# Pretty print the codebook (without descriptions) using pander
pander::pander(codebook[, 1:3], style = "rmarkdown", 
               caption = "Overview of created codebook excluding descriptions")
```

As can be seen, all the features are of the type double except the main diagnosis classification factor.

\newpage
## Loading in the dataset
The data will be loaded in with the `read_csv` function from the `readr` package, this function returns the data as a tibble data frame.
This function allows a vector with column names to be given with an argument, the names from the column `Column Name` of the codebook will be used.
```{r load-data-file}
data <- readr::read_csv("data/raw/wdbc.data", col_names = codebook[[1]], show_col_types = FALSE)

# Print the amount of samples and columns
cat("Amount of samples:", dim(data)[1], "\tColumns in dataframe:", dim(data)[2], "\n")
```
The amount of samples and columns are as expected, so in this aspect the dataset has been read correctly. However, what is more important is if the values are read correctly, since the values are that what is actually going to be used.

```{r remove-tibble-attributes, include=FALSE}
attr(data, "spec") <- NULL; attr(data, "problems") <- NULL
```
```{r print-data-structure}
str(data)
```
Luckily it looks like the values for every column have correctly been read, but there is one thing that could still be changed; the diagnosis column. It would be more helpful if diagnosis was a factor and not just a character column.

```{r data-distribution, out.width = "75%", fig.cap = "Barplot showing distribution of diagnosis classification labels"}
data$diagnosis <- factor(data$diagnosis, labels = c("Benign", "Malignant"))

ggplot(data, aes(x = diagnosis, fill = diagnosis)) + geom_bar() + 
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = -15) + 
  scale_fill_hue(direction = 1, h.start = 180) +
  ggtitle("Barplot of diagnosis classification factor")
```
There are more benign cases than malignant, this means the dataset is not entirely balanced and could cause bias if not handled correctly.

The dataset has the `id` column, this column is not needed for the analysis that will be performed so it will therefore be dropped from the data frame. The `id` column could even cause small a hiccup when creating the machine learning model, since all the values are unique the model could use that as the only feature to predict the classification label. This would of course not work since unseen data will not have the same id's as the data used for training the machine learning algorithm.
```{r drop-id-column}
data <- dplyr::select(data, -id)
cat( sprintf("ID column present?: %s", "id" %in% colnames(data)) )
```

\newpage
## Data inspection
It is important to have a good understanding of what the data is like, for example how the data is distributed and if there is data corruption. A few things that should be checked are if there are any outliers present, any data is skewed (an asymmetry in data distribution) or if there is any missing data.
```{r check-missing-values}
cat("Missing values:", any(is.na(data)))
```
Luckily there are no missing values in the dataset. But now it is good to get an idea of what the values of the columns look like, what ranges do their values fall in? This can be done with the function `summary()` for all columns at the same time, it will create a basic statistics overview about the columns.

### Overall data summary
```{r show-data-summary}
pander::pander(summary(data), caption = "Summary with basic statistics about the data colums")
```
By glancing over the summary created above a few things can be noticed and questions can arise, for example the `area_mean`, `concave_pts_mean`, `radius_se`, `perimeter_se` and `area_worst` columns. These, among other columns, have a wide range of values with their minimum or maximum values far from the quantiles or median.  
This could mean that there are outliers in the data, the questions that arise because of this is if these points are actually outliers and if they should be excluded from the data. It can however not be determined with just the summary above if these are actually outliers and if they need to be removed. 

## Univariate analysis
To check if points are outliers the data needs to be visualized, this can be done by creating a box plot and/or density plot for each of the columns.
Creating them separately for all 30 feature columns would result in 60 plots, which is a lot and perhaps too many. To reduce the amount of total plots, the density plots will instead be visualized using violin plots with the box plots inside of them. Because printing all the data of a feature column in a single box or violin does not show what is desired, the plots will be split on the diagnosis classification factor.

There is not a function in `ggplot` to create these split violin plots, the source code file `src/scripts/split_violin_plot.R` contains a function that uses ggplot as a basis to create split violin plots. Then, using this function from the source file, another function is created that assembles the full plot including the box plot, plot title and themes. After this a list of column names can be given of which a plot will be generated for each and then the plots will be arranged with a common legend to the output file. So there is a clearer overview where the data can be compared the plots for the feature columns will be split on their specification, resulting in three arranged layouts with the mean-, standard error- and worst plots.

```{r create-plot-functions}
getColumnFullName <- function(col_name) {
  return(dplyr::filter(codebook, `Column Name` == col_name)$`Full Name`)
}

createFeaturePlot <- function(col_name, figure_tab) {
  plot <- ggplot(data, aes(x = "", y = !!sym(col_name), fill = diagnosis)) +
    geom_split_violin(alpha = 0.6, trim = FALSE) +
    geom_boxplot(width = 0.2, alpha = 0.6, fatten = NULL, show.legend = F) +
    stat_summary(fun.data = "mean_se", geom = "pointrange", show.legend = F, 
                 position = position_dodge(0.2), size = 0.3) +
    scale_fill_brewer(palette = "Dark2", name = "Diagnosis:") +
    ggtitle( getColumnFullName(col_name) ) +
    theme_minimal() + labs(y = NULL, x = NULL, tag = figure_tab) + 
    theme(plot.title = element_text(size = 9, face = "bold"))
  return(list(plot))
}

createPlotGrid <- function(extension) {
  desired_col_names <- colnames(data)[colnames(data) %like% extension]
  figure_tabs <- letters[1:length(desired_col_names)]
  # Create plots and put them in a list
  plot_list <- mapply(createFeaturePlot, desired_col_names, figure_tabs)
  # Print the plots in an arranged grid with the legend at the bottom
  ggpubr::ggarrange(plotlist = plot_list, ncol = 4, nrow = 3,
                    common.legend = TRUE, legend = "bottom")
}
```

With these functions the spits violin plots can easily be made as three plot grids with the `createPlotGrid()` function, split by the three nuclei features. Then the grids will be annotated with a plot title and then they are done.

\newpage
```{r violin-plots-mean, fig.width = 7.5, fig.height = 9.5, fig.cap = "Split violin plots showing density joined with box plots showing quartiles and outliers, for all 'mean' feature columns"}
# Create split violin plots for all mean feature columns
ggpubr::annotate_figure(
  createPlotGrid("_mean"), 
  top = text_grob("Grid of violin plots with boxplots for each 'mean' feature column",
                  face = "bold", size = 14))
```

```{r violin-plots-se, fig.width = 7.5, fig.height = 9.5, fig.cap = "Split violin plots showing density joined with box plots showing quartiles and outliers, for all 'standard error' feature columns"}
# Create split violin plots for all standard error feature columns
ggpubr::annotate_figure(
  createPlotGrid("_se"), 
  top = text_grob("Grid of violin plots with boxplots for each 'standard error' feature column",
                  face = "bold", size = 14))
```

```{r violin-plots-worst, fig.width = 7.5, fig.height = 9.5, fig.cap = "Split violin plots showing density joined with box plots showing quartiles and outliers, for all 'worst/extreme' feature columns"}
# Create split violin plots for all worst/extreme feature columns
ggpubr::annotate_figure(
  createPlotGrid("_worst"), 
  top = text_grob("Grid of violin plots with boxplots for each 'worst' feature column",
                  face = "bold", size = 14))
```

Looking at the resulting plots it can easily be seen if there is a distinctive correlation between the classification factor and the values of feature columns.
The couple of columns that stand out the most are the `Radius`, `Perimeter`, `Area`, `Concavity` and `Concave Points` feature columns, these all seem to have a clear distinction between the diagnosis classification. One thing that should be noted is that in most of the standard error columns a lot of outliers can be seen, for now nothing will be done with these but it is a good thing to know might problems arise later on.

Now just because it *looks* like there is a clear distinction it needs to be made sure that the difference between the class labels for the feature is actually significant. This can be tested with a 1-way ANOVA test and will be done for the radius_mean and texture_se to show the difference between significance and no significance.
```{r p-value-test}
pval_radius_mean <- summary(aov(radius_mean ~ diagnosis, data = data))[[1]][1,5]
pval_texture_se <-summary(aov(texture_se ~ diagnosis, data = data))[[1]][1,5]

cat("P-Value of radius_mean =", pval_radius_mean,
    "\nP-Value of texture_se  =", pval_texture_se)
```
An alpha of 0.05 is used for the 1-way ANOVA test and when looking at the resulting p-values it is just as expected, the mean radius is significant and the texture standard error is not. So the chance of texture_se being used in an efficient machine learning model is very slim but radius_mean being used is very likely, unless there is a heavily correlated feature column with higher information gain.

\newpage
## Multivariate analysis
A machine learning model should be kept as simple as possible whilst keeping the accuracy still high, this is so model does not become overfitted to the data used for training. Overfitting causes the model to be less accurate on unknown data because specific combinations of many features in the training data might not exist in the test data. So only the couple of features that correlate the highest with the classification factor are desired. But when two features correlate very highly to *each other* and if they are then both used for the model, then these features together does not improve the model any more than if only one of them was used. Because of this it is desired to identify correlations between features because one of the features from the pair should at least not be used for the model.

### Heatmap of correlation matrix
To quickly identify the correlations between all features a correlation matrix is made where all possible pairs of features have their correlation calculated. Correlations can be visualized in a heatmap, where the strength of correlations is shown with a color gradient so they can easily be visually seen.
```{r correlation-matrix}
# Create correlation matrix with only numerical columns and insert feature name column
cor_mat <- tibble::as_tibble( stats::cor(select(data, -diagnosis)) ) %>% 
  dplyr::mutate(col_names = all_of(colnames(.))) %>% 
  dplyr::select(31, 1:30)
# Show first four columns of correlation matrix
pander::pander(head(cor_mat[1:5], n = 4), caption = "Head of wide correlation matrix")
```

Before the correlation matrix can be used to create a heatmap it needs to be converted to long format, this is because of the way R and ggplot read and use data.
```{r long-correlation-matrix}
# Convert data to long format so a heatmap can be made from it
cor_mat_long <- tidyr::pivot_longer(data = cor_mat, cols = all_of(colnames(cor_mat[-1])), 
                                    names_to = "variable", values_to = "pair_cor")
# Show what the data looks like now
pander::pander(head(cor_mat_long, n = 4), caption = "Head of long correlation matrix")
```

In the matrix the features were not physically paired, now in the long format the feature pairs are saved as rows with their correlation score. This way the first two columns, each with the name of a feature of the pair, can be used as the axes of the heatmap plot.

```{r heatmap, fig.cap = "Heatmap visualizing pairwise correlation matrix of all feature colums"}
ggplot(data = cor_mat_long, aes(x = col_names, y = variable, fill = pair_cor)) +
  geom_tile() + labs(x = NULL, y = NULL) +
  scale_fill_gradient(high = "purple", low = "white" ) +
  theme(axis.text.x = element_text(angle = 45, hjust=1)) +
  ggtitle("Heatmap of column correlations")
```
There are two groups of three nuclei features that highly correlate together on all three of their feature columns (mean, standard error and worst).
The first group consists of the radius, perimeter and area nuclei features and the second group consists of the compactness, concavity and concave points features. For both groups only one of the mean, standard error and worst feature columns should be used, so for example only the area- and only the concavity feature columns.  
Another thing that can immediately be noticed and should not be surprising is that the mean-, standard error- and worst feature columns of each nuclei boundary feature, i.e. of the area, also have a higher correlation to each other. So putting together that the area, radius and perimeter features all correlate to each other and themselves only one of the 9 feature columns will probably be used for the model, with the same being the case for the other group.  

There are a few other feature columns that correlate highly together but since there are only 30 total the machine learning algorithms will decide the final ones to be used. So no feature columns will actually be dropped from the data right now, but to show what a correlation between features looks like, two scatterplots will be made of radius_mean plotted against perimeter_mean and fractal_dim_mean.

```{r scatterplots, out.width = "85%", fig.cap = "Scatterplots with trendlines of the Mean Radius and Mean Perimeter features and the Mean Radius and Mean Fractal Dimension features. Showing the difference in data point distribution between correlated and non-correlated features."}
createScatterPlot <- function(plot_title, feature_two) {
  ggplot(data, aes(x = radius_mean, y = !!sym(feature_two), color = diagnosis)) +
    labs(x = "Mean Radius", y = getColumnFullName(feature_two)) +
    geom_jitter(width=0.2, height=0.2, alpha=0.5, shape=16, size=0.8,
                mapping = aes(color = diagnosis)) +
    geom_smooth(formula = y ~ x, method = "loess") + ggtitle(plot_title) +
    scale_color_hue(direction = 1, h.start = 180)
}
scatter1 <- createScatterPlot("Plot of correlated features", "perimeter_mean")
scatter2 <- createScatterPlot("Plot of non-correlated features", "fractal_dim_mean")
my_grid <- ggpubr::ggarrange(scatter1, scatter2, common.legend = T, legend = "bottom")

ggpubr::annotate_figure(my_grid,
  top = text_grob("Scatterplots with trendlines showing the difference\nbetween correlated and non-correlated features",
                  face = "bold", size = 14))
```
As can be seen in *Figure \ref{fig:scatterplots}* , the points of Mean Radius and Mean Perimeter have a clear trend line drawn through them; when the radius increases, so does the perimeter by a certain amount. With one of them the other can be predicted, because of this they do not give a different insight or more information on how to classify an instance. In the second scatterplot of the Mean Radius and the Mean Fractal Dimension it is the opposite, there is no clear pattern showing that if the mean radius increases it is correlated to a linear increase in the mean fractal dimension. 

### Principal Component Analysis
It is helpful to see how and if the data clusters, by creating scatterplots to see the relation between the data points. This is however impossible to plot with the 30 feature columns, or rather dimensions, there are in the dataset. With only two features a two-dimensional plot could be made, but to create a 30-dimensional plot is not possible. It is possible to create lots of plots for all the feature pairs, but this is also not helpful and realistic.  
One of the ways to be able to plot data on a 2D scale to see how the data is related is by creating a Principal Component Analysis (PCA) plot, where the dimensions are reduced by converting the correlations among all samples together into Principal Components. PCA starts by finding the best fitting line by maximizing the sum of squares from the projected points to the origin, which is called Principal Component 1 (PC1). After PC1 is found the next best fitting line that also goes through the origin but is perpendicular to PC1, this line is then subsequently called PC2. This continues until the PCs have been found for all features/dimensions, each going through the origin and being perpendicular to the previous principal components. Then, with all the PCs, the proportion of variance that each PC accounts for can be calculated.

```{r variance-plot, out.width = "75%", fig.cap = "Barplot showing how much variance is explained by each of the first ten principle components of the PCA"}
# PCA of data except diagnosis column
pca_res <- stats::prcomp(data[-1], center = TRUE, scale = TRUE)

# Calculate variance explained from sdev
pc_names <- sprintf("PC%s", seq(1:length(pca_res$sdev)))
var_explained <- data.frame(pc = factor(pc_names, levels = pc_names), 
                            var = pca_res$sdev^2 / sum(pca_res$sdev^2) )

# Create barplot of first 10 PCs with variance converted to %
ggplot(var_explained[1:10,], aes(x = pc, y = var, label = scales::percent(var))) + 
  geom_bar(stat = "identity", fill = "darkturquoise") +
  labs(x = "", y = "Variance Explained", 
       title = "Barplot showing the variances explained by first 10 PCs") +
  scale_y_continuous(labels = scales::percent) + geom_text(nudge_y = 0.015)

# Print total percentage covered by first two and first six PCs
cat(sprintf(" Variance explained by PC1 and PC2: %.1f%% \n", sum(var_explained$var[1:2])*100),
    sprintf( "Variance explained by PC1 through PC6: %.1f%%", sum(var_explained$var[1:6])*100))
```
A decent amount of variance is explained by just the first two principle components, with the first six explaining most variance. In *Figure \ref{fig:variance-plot}* the variance explained by each of the first 10 PCs can be seen.

```{r pca-plot, out.width = "85%", fig.cap = "PCA plot of first two principle components, together accounting for 63.3 percent of variance"}
ggbiplot::ggbiplot(pca_res, obs.scale = 1, var.scale = 1,
                   groups = data$diagnosis, ellipse = FALSE, circle = TRUE) +
  scale_color_discrete(name = "Diagnosis") + ggtitle("Principal Component Analysis Plot")
```
In the PCA plot of *Figure \ref{fig:pca-plot}* the first two PCs can be seen and show a good clustering by diagnosis of the data on the PC1 axis. It can also be seen that there are multiple features that have their directions overlapping, this is because of what already has been discussed, namely that there is a high correlation between those features. 

\newpage
## Creating .arff data file for machine learning experiments in Weka
The dataset does not need to be cleaned any further, there are no missing values and the id column has already been removed. There are a few clusters of columns but to manually decide which should be dropped and which used is not the best way. Since there are only 30 features they will all be passed to the machine learning algorithms and there the best features will be selected. The classification factor level order will be changed so confusion matrices (explained later) are more logical, the column will also be set as the last column, since the last column is the one Weka expects to be the classification column.
```{r saving-data-to-arff-file}
data$diagnosis <- factor(data$diagnosis, levels = c("Malignant", "Benign"))
RWeka::write.arff(select(data, 2:31, 1), file = "data/processed/data.arff")
```

```{r, include = FALSE}
pvalues <- lapply(data[-1], function(x) t.test(x ~ diagnosis, data = data)$p.value) %>%
  unlist() %>% tibble::enframe() %>% dplyr::arrange(-value) %>% 
  "$<-"(name, factor(.$name, levels = .$name)) #%>%
  #"colnames<-"(c("Column Name", "p-value"))

pander::pander(pvalues, caption = "p-values for feature column's t-test between the diagnosis classifications")

ggplot(pvalues, aes(x = -log(value, 2), y = name)) +
  geom_col() +
  labs(x = "-log(2) of significance (higher is more significant)", y = "") +
  ggtitle("Significance of correlation between features and classification class")
```

<!-- (Machine Learning) -->
# Machine Learning with Weka
Now it is known what the data is like it is time to use machine learning algorithms to create a model that classifies instances as benign or malignant. This will all be done in the application [Weka](https://waikato.github.io/weka-site/index.html), which houses a multitude of machine learning algorithms with a plethora of built-in tools for standard machine learning tasks.

<!-- (Relevant quality metrics) -->
## Relevant quality metrics
The default quality metric to measure the performance of an algorithm with is the accuracy, but accuracy is not always the most important or relevant for each project's use case. The accuracy could be less important than the speed of the model in which the classification is made. Or in some cases the data might not already be completely collected, then batch processing is not possible and stream processing should be used. With stream processing the data is continuously collected and processed fast, piece by piece, and is typically meant for when data is needed immediately.

In this case the data is already fully collected and new samples are processed manually by the acting physician, thus the model does not need to look at aspects like speed. Naturally the accuracy of resulting models is important, which describes the fraction of instances that are correctly classified of the total instances. Another potentially important thing for this project is which type of classification errors are more important. It might be preferable to earlier classify an instance as malignant over benign if not completely sure, because if a malignant breast mass gets classified as benign the patient might then make the decision to not get treatment even if they actually do need it. Almost all metrics that an algorithm can be scored by stem from the values recorded in a confusion matrix.

### Confusion Matrix
A standard overview that is created and shown for models is the confusion matrix, in the confusion matrix it can be seen how instances are classified by the model versus what they actually are. For this dataset correctly classified malignant instances are true positive (TP), benign instances classified as malignant are false positive (FP), correctly classified benign instances are true negative (TN) and malignant instances classified as benign are false negative (FN). *Table \ref{tab:confusion-matrix-example}* is an example of a confusion matrix returned by Weka.

\begin{table}[h]
\centering
\begin{tabular}{ r r|l } 
  a & b & <- classified as \\
  \hline
  TP & FN & a = Malignant \\
  FP & TN & b = Benign \\
\end{tabular}
\caption{\label{tab:confusion-matrix-example}Example of a confusion matrix. The columns are what the instances are classified as by the model and the rows show what their classification actually is.}
\end{table}

### Sensitivity and Specificity
Two important metrics for this project, and for machine learning algorithms in general, are sensitivity and specificity. These describe the probability of an instance being correctly classified as their actual classification, calculated using the values from the confusion matrix. The sensitivity is also known as the true positive rate (TPR) and is calculated as $\mathrm{TPR} = \frac{\mathrm{TP}}{ \mathrm{TP} + \mathrm{FN} }$, describing the ability of the model to correctly predict positive instances as positive. Whereas the specificity is also known as the true negative rate (TNR) which describes the ability of the model to correctly predict negative instances as negative and is calculated as $\mathrm{TNR} = \frac{\mathrm{TN}}{ \mathrm{TN} + \mathrm{FP} }$.

### Precision
Because it is preferred for a model to get as few malignant instances classified as benign, meaning that there will be more false positive instances. Precision, or also known as the positive predictive values (PPV) metric, describes the ratio of correctly classified positive instances to the total amount of instances that were classified as positive. Since it is desired for there to be more false positives than false negatives this is not a main metric the algorithms will be scored by but it is still very relevant. Precision is calculated as $\mathrm{PPV} = \frac{ \mathrm{TP} }{ \mathrm{TP} + \mathrm{FP} }$.

### ROC curve and Area under Curve
Receiver Operating Characteristics (ROC) was first developed by electrical engineers and radar engineers in World War II, but it is now also widely used for machine learning to measure the performance of a model with various probability threshold settings. It illustrates the diagnostic ability of a model as it's threshold varies, by plotting the true positive rate on the y axis against the false positive rate on the x axis. The ROC curve can be reduced and normalized to a single number, called the Area under Curve (AUC), which describes the probability of a classifier correctly classifying one random positive instance and one randomly selected negative instance.

The AUC of the ROC curve (AUC-ROC) can thus be used as a clear reference level for comparing algorithm performances. An AUC-ROC value of 1 would mean the model is the most optimal it could be and a value of 0 would mean the least optimal. A value of 0.5 would equate to just as good as a random guess, which is thus the worst case scenario. Even though a very low value close to 0 would not be optimal it does however classify instances consistently the wrong way around, thus if the classification is flipped, the instances are predicted correctly like if the AUC-ROC value would be close to 1. In general, algorithms with AUC-ROC values above 0.7 are considered accepted, above 0.8 are considered excellent and above 0.9 are even considered outstanding.


\newpage
<!-- (How the algorithm performance is tested) -->
## How the performances of machine learning algorithms and their settings will be tested
Manual exploration of algorithms and their settings can be done in the Weka Explorer but to compare a lot of different algorithms with different settings this becomes a tedious task. The Weka Experimenter is the solution to easily compare the different algorithms and their settings by enabling the user to select multiple datasets and also multiple algorithms that can all be run and tested with 10 fold cross-validation, 10 times each. An output file can be specified where each run and each cross validation gets all metrics and statistics saved to. The different algorithms' metrics can then be compared with a paired student's t-test to check for significant improvements or declines in performance. 

Even though the Weka experimenter has the ability to do this, it is only able to do so per metric and the result will the need to be manually copied to this research log. For this reason the script `src/scripts/weka_analyser_custom.R` was created with the same functionality and with the exact same correction method for the t-test as found in the Weka Experimenter Analyse tab.

The resulting data from the Weka Experimenter can be loaded in using the `loadPerformanceData()` function and the comparison can then be made using `createTableWithSignificances()`, to print it in a nice format for the resulting pdf the `printNiceLatexTable()` function is used and `showAllKeys()` is called to show the used settings. Because this will be done for multiple runs this process is encapsulated in the function below for easier use.

```{r print-performance-function, cache = FALSE, results = 'asis'}
printPerformanceFromFile <- function(file_path, caption, print_keys, show_num_false) {
  # Load performance data from file
  performance_data <- loadPerformanceData(file_path)
  
  # Desired comparison fields/metrics
  comp_fields <- c("Percent_correct", "True_positive_rate", "True_negative_rate", 
                   "IR_precision", "Area_under_ROC")
  if (show_num_false) {
    comp_fields <- c(comp_fields, "Num_false_positives", "Num_false_negatives")
  }
  
  # Create table
  comparison_dt <- createTableWithSignificances(performance_df = performance_data,
                                                test_base_selection = 1,
                                                comparison_fields = comp_fields)
  
  # Pretty print with nicer column names
  metric_names <- c("Accuracy", "Sensitivity", "Specificity", 
                    "Precision", "AUC-ROC")
  if (show_num_false) metric_names <- c(metric_names, "FP", "FN")
  
  printNiceLatexTable(comparison_dt, metric_names, caption)
  
  # Show all algorithms with their settings
  if (print_keys) showAllKeys(performance_data)
}
```

In this case the following five metrics are the most relevant and important and shall be shown and compared, namely the accuracy, sensitivity, specificity, precision and AUC-ROC. To provide some sort of a baseline performance the ZeroR and OneR algorithms are included, since they are very basic and do not use a lot of logic and computation, they can thus be used to benchmark other algorithms with to see if they are better than the baseline performance.


\newpage
<!-- (Standard algorithms' default performance) -->
## Machine Learning algorithms' default performance
The algorithms ZeroR and OneR are included to show a sort of baseline performance, these can be used to benchmark and compare other algorithms to. The other standard machine learning algorithms that will be used and tested are the following: Na√Øve Bayes, Simple Logistic, SVM (SMO), Nearest Neighbor (IBk), Decision Trees (J48) and Random Forest.

\begingroup \fontsize{8pt}{12pt}\selectfont
```{r default-performances, cache = FALSE, results = 'asis'}
file_path <- "output/algorithm_performances/default.arff"
caption <- "Default performances of standard machine learning algorithms compared to ZeroR"
printPerformanceFromFile(file_path, caption, T, F)
```
\endgroup
```

Based on the performances with default settings seen in *Table \ref{tab:default-settings}*, the SimpleLogistic and SMO algorithms look to be the best performing, with RandomForest as a close third. The algorithms could still be over-fitted and the J48 tree too big, so some settings will be changed to try to improve the resulting models. For now only OneR and J48 will be dropped and further tests will be performed on the remaining algorithms.


\newpage
<!-- (Testing algorithm settings with Weka experimenter) -->
## Testing algorithm performances using different settings with Weka experimenter
The effect of different algorithm settings with the goal of improving algorithm performance, on at least 2 machine learning algorithms. Also investigate the effect of Attribute Selection methods.  
Applying appropriate statistical tests (with Weka Experimenter) taking into account the quality metrics specified earlier. Investigate some Meta learners (Stacking, Bagging, Boosting).

<!-- (ROC and learning curve analysis) -->
## ROC and learning curve analysis
### ROC curve visualization
Visualization of one or two final algorithms with optimal settings and explaining if the result is satisfying. Takes into account the quality metrics defined previously.

### Learning curve
How much data is needed to get a reasonable performance estimate?


\newpage
<!-- (Java Wrapper) -->
# Java Wrapper for final learned model
Intens man.


<!-- (References) -->
\newpage
\begin{thebibliography}{9}

\bibitem{street93}
W.N. Street, W.H. Wolberg and O.L. Mangasarian. (1993), \textit{Nuclear feature extraction for breast tumor diagnosis.}, 1993 International Symposium on Electronic Imaging: Science and Technology, volume 1905, pages 861-870, \url{https://doi.org/10.1117/12.148698} (accessed Sep 16, 2022).

\bibitem{mangasarian95}
O.L. Mangasarian, W.N. Street and W.H. Wolberg. (1995), \textit{Breast cancer diagnosis and prognosis via linear programming}, Operations Research, volume 43, issue 4, pages 570-577, \url{https://doi.org/10.1287/opre.43.4.570} (accessed Sep 17, 2022).
\end{thebibliography}